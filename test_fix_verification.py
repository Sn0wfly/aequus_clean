#!/usr/bin/env python3
"""
VERIFICACIÃ“N RÃPIDA: Â¿Se arreglÃ³ el bug de CFR?
Debe mostrar muchos mÃ¡s info sets being trained ahora.
"""

import jax
import jax.numpy as jnp
from poker_bot.core.trainer import (
    PokerTrainer, TrainerConfig, _jitted_train_step,
    unified_batch_simulation, compute_advanced_info_set
)

def test_bug_fix():
    """Test rÃ¡pido para verificar que el bug estÃ¡ arreglado"""
    print("ğŸ”§ VERIFICANDO FIX: Bug de acumulaciÃ³n de regrets")
    print("="*50)
    
    config = TrainerConfig()
    config.batch_size = 16  # Un poco mÃ¡s grande para ver el efecto
    
    # Estado inicial
    regrets = jnp.zeros((config.max_info_sets, config.num_actions), dtype=jnp.float32)
    strategy = jnp.ones((config.max_info_sets, config.num_actions), dtype=jnp.float32) / config.num_actions
    
    key = jax.random.PRNGKey(42)
    
    print(f"ğŸ“Š ConfiguraciÃ³n:")
    print(f"   - Batch size: {config.batch_size}")
    print(f"   - MÃ¡ximo teÃ³rico info sets visitados: {config.batch_size * 6} (batch Ã— jugadores)")
    
    # Ejecutar UN paso de CFR
    print(f"\nğŸ”„ Ejecutando UN paso de CFR corregido...")
    
    new_regrets, new_strategy = _jitted_train_step(regrets, strategy, key)
    
    # Analizar cuÃ¡ntos info sets fueron entrenados
    positive_regrets = jnp.maximum(new_regrets, 0.0)
    regret_sums = jnp.sum(positive_regrets, axis=1, keepdims=True)
    
    info_sets_trained = jnp.sum(regret_sums > 1e-6)
    theoretical_max = config.batch_size * 6  # 6 jugadores por juego
    
    print(f"\nğŸ“ˆ RESULTADOS:")
    print(f"   - Info sets entrenados: {info_sets_trained}")
    print(f"   - TeÃ³rico mÃ¡ximo: {theoretical_max}")
    print(f"   - Cobertura: {float(info_sets_trained)/theoretical_max:.2%}")
    print(f"   - MÃ¡ximo regret sum: {jnp.max(regret_sums):.3f}")
    
    # Verificar si el fix funcionÃ³
    expected_minimum = theoretical_max * 0.5  # Al menos 50% de cobertura esperada
    
    if info_sets_trained >= expected_minimum:
        print(f"\nâœ… Â¡FIX EXITOSO!")
        print(f"   - Antes: ~18 info sets por batch")
        print(f"   - Ahora: {info_sets_trained} info sets por batch")
        print(f"   - Mejora: ~{info_sets_trained/18:.1f}x mÃ¡s info sets entrenados")
        return True
    else:
        print(f"\nâŒ Fix no funcionÃ³ completamente")
        print(f"   - Esperado: >= {expected_minimum}")
        print(f"   - Actual: {info_sets_trained}")
        return False

def test_aa_vs_72o_learning():
    """Test especÃ­fico: Â¿Pueden AA y 72o aprender estrategias diferentes ahora?"""
    print(f"\nğŸƒ TEST ESPECÃFICO: AA vs 72o")
    print("="*40)
    
    config = TrainerConfig()
    config.batch_size = 64  # Batch mÃ¡s grande para mayor probabilidad de entrenar estas manos
    
    trainer = PokerTrainer(config)
    
    # Entrenar por unas pocas iteraciones
    print("â³ Entrenando por 10 iteraciones rÃ¡pidas...")
    
    for i in range(10):
        key = jax.random.PRNGKey(i + 100)
        trainer.regrets, trainer.strategy = _jitted_train_step(
            trainer.regrets, trainer.strategy, key
        )
    
    # Verificar estrategias de AA vs 72o
    from poker_bot.core.trainer import compute_mock_info_set
    
    aa_info = compute_mock_info_set([12, 12], False, 2)  # AA middle position
    trash_info = compute_mock_info_set([5, 0], False, 2)  # 72o middle position
    
    if aa_info < config.max_info_sets and trash_info < config.max_info_sets:
        aa_strategy = trainer.strategy[aa_info]
        trash_strategy = trainer.strategy[trash_info]
        
        aa_aggression = float(jnp.sum(aa_strategy[3:6]))  # BET/RAISE/ALLIN
        trash_aggression = float(jnp.sum(trash_strategy[3:6]))
        
        aa_fold = float(aa_strategy[0])
        trash_fold = float(trash_strategy[0])
        
        print(f"ğŸ¯ RESULTADOS:")
        print(f"   AA strategy: FOLD={aa_fold:.3f}, AGG={aa_aggression:.3f}")
        print(f"   72o strategy: FOLD={trash_fold:.3f}, AGG={trash_aggression:.3f}")
        
        strategies_different = abs(aa_aggression - trash_aggression) > 0.01
        
        if strategies_different:
            print(f"\nâœ… Â¡Ã‰XITO! AA y 72o tienen estrategias diferentes")
            print(f"   - Diferencia en agresiÃ³n: {aa_aggression - trash_aggression:+.3f}")
            return True
        else:
            print(f"\nâš ï¸ AÃºn muy similares, pero esto es normal para 10 iteraciones")
            print(f"   - Diferencia en agresiÃ³n: {aa_aggression - trash_aggression:+.3f}")
            return False
    else:
        print(f"âŒ Error: Info sets fuera de rango")
        return False

def main():
    """Ejecutar verificaciones"""
    print("ğŸš¨ VERIFICACIÃ“N DE FIX CFR")
    print("="*60)
    
    # Test 1: Verificar que mÃ¡s info sets se entrenan
    try:
        fix_works = test_bug_fix()
    except Exception as e:
        print(f"âŒ Error en test_bug_fix: {e}")
        fix_works = False
    
    # Test 2: Verificar que AA vs 72o pueden diferenciarse
    if fix_works:
        try:
            aa_test = test_aa_vs_72o_learning()
        except Exception as e:
            print(f"âŒ Error en test_aa_vs_72o: {e}")
            aa_test = False
    else:
        aa_test = False
    
    print(f"\nğŸ¯ RESUMEN:")
    print(f"   - Fix de info sets: {'âœ…' if fix_works else 'âŒ'}")
    print(f"   - AA vs 72o test: {'âœ…' if aa_test else 'âš ï¸ '}")
    
    if fix_works:
        print(f"\nğŸ‰ Â¡El fix estÃ¡ funcionando! Puedes proceder con el entrenamiento completo.")
    else:
        print(f"\nğŸ”§ El fix necesita mÃ¡s trabajo.")

if __name__ == "__main__":
    main() 